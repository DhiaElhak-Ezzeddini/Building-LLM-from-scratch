{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a77d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preparing the dataset for supervised instruction fine-tuning \n",
    "import json\n",
    "import os \n",
    "import urllib\n",
    "import urllib.request\n",
    "import urllib.response\n",
    "\n",
    "def download_load_file(file_path , url) : \n",
    "    if not os.path.exists(file_path) : \n",
    "        with urllib.request.urlopen(url) as response :\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path , \"w\" , encoding=\"utf-8\") as file :\n",
    "            file.write(text_data)\n",
    "    else : \n",
    "        with open(file_path , \"r\" , encoding=\"utf-8\") as file : \n",
    "            text_data = file.read()\n",
    "    with open(file_path , \"r\")as file  :\n",
    "        data = json.load(file)\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\n",
    "\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "\"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    ")\n",
    "file_path = \"instruction-data.json\"\n",
    "\n",
    "data = download_load_file(file_path , url=url)\n",
    "print(f\"Number of entries : {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a8ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" \n",
    "Example of data : {'instruction': 'Identify the correct spelling of the following word.', \n",
    "                    'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n",
    "\"\"\"\n",
    "print(f\"Example of data : {data[50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caabe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting the entries inn the dataset into Alpaca Style \n",
    "def format_into_alpaca(entry) : \n",
    "    instruction_text =(\n",
    "        f\"Below is an instruction that describes a task. \" \n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input_text =(\n",
    "        f\"\\n\\n### Input:\\n{entry['input']}\"\n",
    "    )\n",
    "    output_text = (\n",
    "        f\"\\n\\n### Response:\\n{entry[\"output\"]}\"\n",
    "    )\n",
    "    return instruction_text + input_text , output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ef4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input , target_output = format_into_alpaca(data[50])\n",
    "print(model_input + target_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f07b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = int(len(data) * 0.85)\n",
    "val_portion = int(len(data) * 0.1)\n",
    "test_portion = len(data) - train_portion - val_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "val_data = data[train_portion:train_portion + val_portion]\n",
    "test_data = data[train_portion + val_portion:]\n",
    "print(f\"training set length   : {len(train_data)}\")\n",
    "print(f\"validation set length : {len(val_data)}\")\n",
    "print(f\"test set length       : {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43d7d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### organizing data into training batches : \n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset) : \n",
    "    def __init__(self,data,tokenizer) : \n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoded_texts = []\n",
    "        for entry in data : \n",
    "            input_inst , target = format_into_alpaca(entry)\n",
    "            full_text = input_inst + target\n",
    "            self.encoded_texts.append(tokenizer.encode(full_text))\n",
    "    def __len__(self) : \n",
    "        return len(self.encoded_texts)\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578be0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "treated_data = InstructionDataset(train_data , tokenizer)\n",
    "print(treated_data[0])\n",
    "print(f\"\\nraw data :\\n\",tokenizer.decode(treated_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_1(batch,pad_token_id=50256,device=\"cpu\") : \n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_lst = []\n",
    "    for item in batch : \n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id ] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d8b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "batch = (\n",
    "inputs_1,\n",
    "inputs_2,\n",
    "inputs_3\n",
    ")\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6419a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(batch,pad_token_id=50256,device=\"cpu\") : \n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_lst , targets_lst = [] , []\n",
    "    for item in batch : \n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id ] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor , targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e06d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs , targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193474d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"    \n",
    "tensor([[    1,     2,     3,     4, 20256],      in order to prevent the padding tokens from contributing to Loss calculation during training \n",
    "        [    6, 20256, 20256, 20256, 20256],      we replace them with a placeholder value -100   \n",
    "        [    8,     9, 20256, 20256, 20256]])     ===> only meaningful tokens can contribute to the loss \n",
    "        \n",
    "tensor([[    1,     2,     3,     4, 20256],\n",
    "        [    6, 20256, -100, -100, -100],\n",
    "        [    8,     9, 20256, -100, -100]])\n",
    "\n",
    "\"\"\"\n",
    "def custom_collate_draft_fn(batch,pad_token_id=50256,device=\"cpu\",ignore_index=-100,allowed_max_length=None) : \n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_lst , targets_lst = [] , []\n",
    "    for item in batch : \n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = (\n",
    "            new_item + [pad_token_id ] * (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "        \n",
    "        \n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1 : \n",
    "            targets[indices[1:]] = ignore_index ## replace all placeholders with -100 except the first one\n",
    "        if allowed_max_length is not None : \n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor , targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f1dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs , targets = custom_collate_draft_fn(batch)\n",
    "print(inputs)\n",
    "print(f\"\\ntargets:\")\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3cd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f704fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "customized_collate_draft_fn = partial(\n",
    "    custom_collate_draft_fn,\n",
    "    device=device,\n",
    "    allowed_max_length = 1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating DataLoaders for instruction dataset : \n",
    "from torch.utils.data import DataLoader\n",
    "num_workers = 0\n",
    "batch_size  = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data,tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=custom_collate_draft_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "val_dataset = InstructionDataset(val_data,tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=custom_collate_draft_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "test_dataset = InstructionDataset(test_data,tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=custom_collate_draft_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83fea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train Loader : \")\n",
    "i=0\n",
    "for inputs,targets in train_loader : \n",
    "    print(f\"Inputs shape : \",inputs.shape,f\"-- Targets shape : \",targets.shape)\n",
    "    i += 1\n",
    "    if i==5 : break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6704698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from modules import GPTModel\n",
    "from modules import load_weights_into_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e671ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_model = \"gpt2_medium (355M)\"\n",
    "input_prompt = \"Every effort moves you\"\n",
    "base_config = {\n",
    "    \"vocab_size\":50257,\n",
    "    \"context_length\":1024,\n",
    "    \"drop_rate\":0.0,\n",
    "    \"qkv_bias\":True\n",
    "}\n",
    "\n",
    "models_config = {\n",
    "    \"gpt2_small (124M)\" : {\"emb_dim\":768 , \"n_layers\":12 ,\"n_heads\":12} , \n",
    "    \"gpt2_medium (355M)\" : {\"emb_dim\":1024 , \"n_layers\":24 ,\"n_heads\":16} , \n",
    "    \"gpt2_large (774M)\" : {\"emb_dim\":1280 , \"n_layers\":36,\"n_heads\":20} , \n",
    "    \"gpt2_xl (1558M)\" : {\"emb_dim\":1600 , \"n_layers\":48 ,\"n_heads\":25} , \n",
    "}\n",
    "base_config.update(models_config[chosen_model])\n",
    "print(base_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03feb68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = chosen_model.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "print(f\"model size : \",model_size)\n",
    "settings , params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef53bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = GPTModel(base_config)\n",
    "load_weights_into_gpt(gpt,params)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc1aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "input_text , target_text = format_into_alpaca(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import generate , text_to_token_ids , ids_token_to_text\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(input_text,tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=base_config[\"context_length\"],\n",
    "    eos_id=50256\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Generated text : \\n{ids_token_to_text(token_ids , tokenizer)}\")\n",
    "### the generate function was originally used in the pre-training stage so it combines the model input and output \n",
    "### since the  original task in pre-training is text completion  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = ids_token_to_text(token_ids,tokenizer)[len(input_text):].strip()\n",
    "print(response_text) ## here we gave only the necessary output which is the response to the input instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb7209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import modules\n",
    "reload(modules)\n",
    "\n",
    "from modules import train_model_simple , calc_loss_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d4539",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader,gpt,device,num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader,gpt,device,num_batches=5)\n",
    "print(f\"Training Loss   : {train_loss:.3f}\")\n",
    "print(f\"Validation Loss : {val_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ee99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "    Training will be done on a Kaggle , than we wil download the weights and complete the work here \n",
    "\"\"\"\n",
    "import time \n",
    "start_context  , target_context= format_into_alpaca(val_data[0])\n",
    "start_time = time.time()\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(),lr=0.00005,weight_decay=0.1)\n",
    "num_epochs = 5\n",
    "train_losses , val_losses , tokens_seen = train_model_simple(\n",
    "    model=gpt,train_loader=train_loader,\n",
    "    val_loader=val_loader,optimizer=optimizer,\n",
    "    device=device,num_epochs=num_epochs,\n",
    "    eval_freq=50,eval_iter=1,\n",
    "    start_context=start_context,tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "exec_time = (end_time - start_time) / 60\n",
    "print(f\"Execution time : {exec_time:.3f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fin_tuned = GPTModel(base_config)\n",
    "state_dict= torch.load(\"instruction.pth\" , weights_only=True)\n",
    "model_fin_tuned.load_state_dict(state_dict)\n",
    "model_fin_tuned.to(device)\n",
    "model_fin_tuned.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca02a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in test_data[:3] : \n",
    "    input_text , target = format_into_alpaca(entry)\n",
    "    token_ids = generate(\n",
    "    model=model_fin_tuned,\n",
    "    idx=text_to_token_ids(input_text,tokenizer).to(device),\n",
    "    max_new_tokens=35,\n",
    "    context_size=base_config[\"context_length\"],\n",
    "    eos_id=50256\n",
    "    )\n",
    "    gen_text = ids_token_to_text(token_ids ,tokenizer)\n",
    "\n",
    "    response_text = (gen_text[len(input_text):].replace(\"### Response:\", \"\").strip())\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
